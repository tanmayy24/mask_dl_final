{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimVP Mask Predictor Model\n",
    "\n",
    "> simvp_mask_predictor_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mask_simvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from openstl.models.simvp_model import SimVP_Model\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DEFAULT_MODEL_CONFIG = {\n",
    "    # For MetaVP models, the most important hyperparameters are: \n",
    "    # N_S, N_T, hid_S, hid_T, model_type\n",
    "    'in_shape': [11, 3, 160, 240],\n",
    "    'hid_S': 64,\n",
    "    'hid_T': 512,\n",
    "    'N_S': 4,\n",
    "    'N_T': 8,\n",
    "    'model_type': 'gSTA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVP(nn.Module):\n",
    "    def __init__(self, in_shape, hid_S, hid_T, N_S, N_T, model_type, pre_seq_len=11, aft_seq_len=11, drop_path=0.0, downsample=False):\n",
    "        super().__init__()\n",
    "        c = in_shape[1]\n",
    "        self.simvp = SimVP_Model(\n",
    "            in_shape=in_shape, hid_S=hid_S, \n",
    "            hid_T=hid_T, N_S=N_S, N_T=N_T, \n",
    "            model_type=model_type, drop_path=drop_path)\n",
    "        self.token_embeddings = nn.Embedding(49, c)\n",
    "        self.out_conv = nn.Conv2d(c, 49, 1, 1)\n",
    "        self.pre_seq_len = pre_seq_len\n",
    "        self.aft_seq_len = aft_seq_len\n",
    "        self.downsample = downsample\n",
    "        self.down_conv = nn.Conv2d(c, c, kernel_size=3, stride=2, padding=1)\n",
    "        self.up_conv = nn.ConvTranspose2d(c, c, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.token_embeddings(tokens)\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "        if self.downsample:\n",
    "            b, t, *_ = x.shape\n",
    "            x = x.view(b*t, *x.shape[2:])\n",
    "            x = self.down_conv(x)\n",
    "            x = x.view(b, t, *x.shape[1:])\n",
    "\n",
    "        if self.aft_seq_len == self.pre_seq_len:\n",
    "            y_hat = self.simvp(x)\n",
    "        elif self.aft_seq_len < self.pre_seq_len:\n",
    "            y_hat = self.simvp(x)\n",
    "            y_hat = y_hat[:, :self.aft_seq_len]\n",
    "        elif self.aft_seq_len > self.pre_seq_len:\n",
    "            d = self.aft_seq_len // self.pre_seq_len\n",
    "            m = self.aft_seq_len % self.pre_seq_len\n",
    "    \n",
    "            y_hat = []\n",
    "            cur_seq = x.clone()\n",
    "            for _ in range(d):\n",
    "                cur_seq = self.simvp(cur_seq)\n",
    "                y_hat.append(cur_seq)\n",
    "            \n",
    "            if m != 0:\n",
    "                cur_seq = self.simvp(cur_seq)\n",
    "                y_hat.append(cur_seq[:, :m])\n",
    "            \n",
    "            y_hat = torch.cat(y_hat, dim=1)\n",
    "\n",
    "        b, t, *_ = y_hat.shape\n",
    "        y_hat = y_hat.view(b*t, *y_hat.shape[2:])\n",
    "        if self.downsample:\n",
    "            y_hat = self.up_conv(y_hat)\n",
    "\n",
    "        y_hat_logits = self.out_conv(y_hat)\n",
    "\n",
    "        _, _, h, w = y_hat_logits.shape\n",
    "        y_hat_logits = y_hat_logits.view(b, t, 49, h, w)\n",
    "        return y_hat_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_seq_len=11\n",
    "aft_seq_len=1\n",
    "model = MaskSimVP(**DEFAULT_MODEL_CONFIG, downsample=True, pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randint(0, 49, (1, pre_seq_len, 160, 240)).long()\n",
    "out = model(x)\n",
    "assert out.shape == (1, aft_seq_len, 49, 160, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
