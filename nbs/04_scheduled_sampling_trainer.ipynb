{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Trainer for MaskSimVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scheduled_sampling_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import lightning as pl\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from maskpredformer.mask_simvp import MaskSimVP\n",
    "from maskpredformer.vis_utils import show_gif \n",
    "from maskpredformer.simvp_dataset import DLDataset, ValMetricDLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def inv_sigmoid_schedule(x, n, k):\n",
    "    y = k / (k+math.exp(((x-(n//2))/(n//20))/k))\n",
    "    return y\n",
    "\n",
    "def exp_schedule(x, n, k=np.e):\n",
    "    t = 100 * np.maximum((x / n)-0.033,0)\n",
    "    return k ** -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVPScheduledSamplingModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 in_shape, hid_S, hid_T, N_S, N_T, model_type,\n",
    "                 batch_size, lr, weight_decay, max_epochs, data_root, use_gt_data,\n",
    "                 sample_step_inc_every_n_epoch, schedule_k=1.05, max_sample_steps=5,\n",
    "                 schedule_type=\"exponential\", load_datasets=True,\n",
    "                 pre_seq_len=11, aft_seq_len=1, drop_path=0.0, unlabeled=False, downsample=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MaskSimVP(\n",
    "            in_shape, hid_S, hid_T, N_S, N_T, model_type, downsample=downsample, drop_path=drop_path,\n",
    "            pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len\n",
    "        )\n",
    "        if load_datasets:\n",
    "            self.train_set = DLDataset(data_root, \"train\", unlabeled=unlabeled, use_gt_data=use_gt_data, pre_seq_len=pre_seq_len, aft_seq_len=max_sample_steps+1)\n",
    "            self.val_set = ValMetricDLDataset(data_root)\n",
    "            self.schedule_max = (len(self.train_set)//batch_size) * sample_step_inc_every_n_epoch\n",
    "            print(f\"Schedule max: {self.schedule_max}\")\n",
    "        else:\n",
    "            self.schedule_max = -1 # dummy value\n",
    "        \n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.iou_metric = JaccardIndex(task='multiclass', num_classes=49)\n",
    "        \n",
    "        self.schedule_idx = 0\n",
    "        self.sample_steps = 1\n",
    "        self.sampled_count = 0\n",
    "\n",
    "    def get_p(self):\n",
    "        if self.hparams.schedule_type == \"exponential\":\n",
    "            p = 1-exp_schedule(self.schedule_idx, self.schedule_max, self.hparams.schedule_k)\n",
    "        elif self.hparams.schedule_type == \"inverse_sigmoid\":\n",
    "            p = 1 - inv_sigmoid_schedule(self.schedule_idx, self.schedule_max, self.hparams.schedule_k)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Schedule type {self.hparams.schedule_type} not implemented\")\n",
    "        return p\n",
    "\n",
    "    def sample_or_not(self):\n",
    "        assert self.schedule_idx < self.schedule_max, \"Schedule idx larger than max, something wrong with schedule\"\n",
    "        p = self.get_p()\n",
    "        self.schedule_idx += 1\n",
    "        return random.random() < p\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_autoregressive(self, x, t):\n",
    "        cur_seq = x.clone()\n",
    "        for _ in range(t):\n",
    "            y_hat_logit_t = self.model(cur_seq)\n",
    "            y_hat = torch.argmax(y_hat_logit_t, dim=2) # get current prediction\n",
    "            cur_seq = torch.cat([cur_seq[:, 1:], y_hat], dim=1) # autoregressive concatenation\n",
    "        return cur_seq\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if self.sample_or_not():\n",
    "            self.sampled_count += 1\n",
    "            x = self.sample_autoregressive(x, self.sample_steps)\n",
    "            y = y[:, self.sample_steps:self.sample_steps+1] # get the next label after sampling model `sample_steps` times\n",
    "        else:\n",
    "            # no change in x\n",
    "            y = y[:, 0:1] # get the normal training label\n",
    "        \n",
    "        y_hat_logits = self.model(x)\n",
    "        \n",
    "        # Flatten batch and time dimensions\n",
    "        b, t, *_ = y_hat_logits.shape\n",
    "        y_hat_logits = y_hat_logits.view(b*t, *y_hat_logits.shape[2:])\n",
    "        y = y.view(b*t, *y.shape[2:])\n",
    "\n",
    "        loss = self.criterion(y_hat_logits, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        if self.logger:\n",
    "            self.logger.log_metrics(\n",
    "                {\n",
    "                    \"sample_steps\": self.sample_steps,\n",
    "                    \"schedule_idx\": self.schedule_idx,\n",
    "                    \"schedule_prob\": self.get_p(),\n",
    "                    \"sampled_count\": self.sampled_count,\n",
    "                }\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.sample_autoregressive(x, 11)\n",
    "        iou = self.iou_metric(y_hat[:, -1], y[:, -1])\n",
    "        self.log(\"valid_last_frame_iou\", self.iou_metric, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return iou\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if (self.current_epoch+1) % self.hparams.sample_step_inc_every_n_epoch == 0:\n",
    "            print(\"Increasing sample steps\")\n",
    "            self.schedule_idx = 0\n",
    "            self.sample_steps = min(self.sample_steps+1, self.hparams.max_sample_steps)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test out the MaskSimVP Scheduled Sampling Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/simvp_epoch=16-val_loss=0.014.ckpt\"\n",
    "mask_sim_vp_ckpt = torch.load(ckpt_path)\n",
    "\n",
    "ss_params = mask_sim_vp_ckpt['hyper_parameters']\n",
    "ss_params['unlabeled'] = False\n",
    "ss_params['sample_step_inc_every_n_epoch'] = 2\n",
    "ss_params['max_epochs'] = 10\n",
    "ss_params['max_sample_steps'] = 5\n",
    "ss_params['use_gt_data'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_module = MaskSimVPScheduledSamplingModule(**ss_params, load_datasets=True)\n",
    "pl_module.load_state_dict(mask_sim_vp_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pl_module(pl_module):\n",
    "    x, y = pl_module.train_set[0]\n",
    "    pl_module.schedule_idx = pl_module.schedule_max * 0.95\n",
    "    pl_module.sample_steps = 1\n",
    "    x = x.unsqueeze(0).to(pl_module.device)\n",
    "    y = y.unsqueeze(0).to(pl_module.device)\n",
    "    loss = pl_module.training_step((x,y), 0)\n",
    "    print(loss)\n",
    "test_pl_module(pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_val_step(pl_module):\n",
    "    x, y = pl_module.val_set[0]\n",
    "    x = x.unsqueeze(0).to(pl_module.device)\n",
    "    y = y.unsqueeze(0).to(pl_module.device)\n",
    "    iou = pl_module.validation_step((x,y), 0)\n",
    "    print(iou)\n",
    "test_val_step(pl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Video Callback\n",
    "\n",
    "> sample video callback to generate video samples during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SampleAutoRegressiveVideoCallback(pl.Callback):\n",
    "    def __init__(self, val_set, video_path=\"./val_videos/\"):\n",
    "        super().__init__()\n",
    "        self.val_set = val_set\n",
    "        self.val_count = 0\n",
    "        self.val_path = video_path\n",
    "        if not os.path.exists(self.val_path):\n",
    "            os.makedirs(self.val_path)\n",
    "\n",
    "    def generate_video(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        sample_idx = random.randint(0, len(self.val_set)-1)\n",
    "\n",
    "        x, y = self.val_set[sample_idx]\n",
    "        x = x.unsqueeze(0).to(pl_module.device)\n",
    "        y = y.unsqueeze(0).to(pl_module.device)\n",
    "        \n",
    "        cur_seq = pl_module.sample_autoregressive(x, 11)\n",
    "\n",
    "        # convert to numpy\n",
    "        x = x.squeeze(0).cpu().numpy()\n",
    "        y = y.squeeze(0).cpu().numpy()\n",
    "        y_hat = cur_seq.squeeze(0).cpu().numpy()\n",
    "\n",
    "        gif_path = os.path.join(self.val_path, f\"val_ar_video_{self.val_count}.gif\")\n",
    "\n",
    "        show_gif(x, y, y_hat, out_path=gif_path)\n",
    "        self.val_count += 1\n",
    "\n",
    "        return gif_path\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_rank == 0:\n",
    "            gif_path = self.generate_video(pl_module)\n",
    "            trainer.logger.experiment.log({\n",
    "                \"val_video\": wandb.Video(gif_path, fps=4, format=\"gif\")\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test video callback\n",
    "sample_video_cb = SampleAutoRegressiveVideoCallback(pl_module.val_set)\n",
    "gif_path = sample_video_cb.generate_video(pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
