{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd11f12-b7a3-492a-ba8a-3e3ccb05c0c9",
   "metadata": {},
   "source": [
    "# Train predictor using autoregressive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196642d1-bd27-4260-918d-e0a5176e8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp autoregressive_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea93c7d-3eed-4c3a-91ec-fd15e28f986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from maskpredformer.mask_simvp import MaskSimVP\n",
    "from maskpredformer.simvp_dataset import DLDataset\n",
    "from maskpredformer.vis_utils import show_video_line, show_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85676b-7295-4ab0-9a18-7e60661139c2",
   "metadata": {},
   "source": [
    "## MaskSimVPAutoRegressiveModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa323b3-0790-4a7f-80e8-d0d3fac02232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVPAutoRegressiveModule(pl.LightningModule):\n",
    "    def __init__(self, in_shape, hid_S, hid_T, N_S, N_T, model_type,\n",
    "                 batch_size, lr, weight_decay, max_epochs,\n",
    "                 data_root, backprop_indices = [10], pre_seq_len=11, aft_seq_len=1,\n",
    "                 drop_path=0.0, unlabeled=False, downsample=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MaskSimVP(\n",
    "            in_shape, hid_S, hid_T, N_S, N_T, model_type, downsample=downsample, drop_path=drop_path,\n",
    "            pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len\n",
    "        )\n",
    "        self.backprop_indices = backprop_indices\n",
    "        self.train_set = DLDataset(data_root, \"train\", unlabeled=unlabeled, pre_seq_len=11, aft_seq_len=11)\n",
    "        self.val_set = DLDataset(data_root, \"val\", pre_seq_len=11, aft_seq_len=11)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def calculate_loss(self, logits, target):\n",
    "        b, t, *_ = logits.shape\n",
    "        logits = logits.view(b*t, *logits.shape[2:])\n",
    "        target = target.view(b*t, *target.shape[2:])\n",
    "        loss = self.criterion(logits, target)\n",
    "        return loss\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        y_hat_logits = []\n",
    "        cur_seq = x.clone()\n",
    "        for i in range(11):\n",
    "            y_hat_logit_t = self.model(cur_seq)\n",
    "            if i in self.backprop_indices:\n",
    "                y_hat_logits.append(y_hat_logit_t) # get logits for backprop\n",
    "            y_hat = torch.argmax(y_hat_logit_t, dim=2) # get current prediction\n",
    "            cur_seq = torch.cat([cur_seq[:, 1:], y_hat], dim=1) # autoregressive concatenation\n",
    "        \n",
    "        y_hat_logits = torch.cat(y_hat_logits, dim=1)\n",
    "        assert y_hat_logits.size(1) == len(self.backprop_indices)\n",
    "        # calculate loss\n",
    "        loss = self.calculate_loss(y_hat_logits, y[:, self.backprop_indices])\n",
    "        del y_hat_logits\n",
    "        return loss, cur_seq\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, _ = self.step(x, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        loss, _ = self.step(x, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.hparams.lr,\n",
    "            total_steps=self.hparams.max_epochs*len(self.train_dataloader()),\n",
    "            final_div_factor=1e4\n",
    "        )\n",
    "        opt_dict = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\":{\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            } \n",
    "        }\n",
    "\n",
    "        return opt_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8aa6c-781e-4dd0-b351-68d8071ac331",
   "metadata": {},
   "source": [
    "**Test out the MaskSimVPAutoRegressive Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fee37-ae76-4bf0-b2c7-69fa1bdde957",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1e799-4011-4340-85af-7487c352cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"checkpoints/in_shape=11-49-160-240_hid_S=64_hid_T=512_N_S=4_N_T=8_model_type=gSTA_batch_size=4_lr=0.001_weight_decay=0.0_max_epochs=20_pre_seq_len=11_aft_seq_len=1_unlabeled=True_downsample=True/simvp_epoch=16-val_loss=0.014.ckpt\"\n",
    "mask_sim_vp_ckpt = torch.load(ckpt_path)\n",
    "\n",
    "autoregressive_params = mask_sim_vp_ckpt['hyper_parameters']\n",
    "autoregressive_params['unlabeled'] = False\n",
    "\n",
    "pl_module = MaskSimVPAutoRegressiveModule(**autoregressive_params)\n",
    "pl_module.load_state_dict(mask_sim_vp_ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7509d7-f5f5-47bc-9ebb-465f1b879640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prior_model_results():\n",
    "    x, y = pl_module.val_set[0]\n",
    "    x=x.unsqueeze(0).to(pl_module.device); y=y.unsqueeze(0).to(pl_module.device)\n",
    "    return y, *pl_module.step(x, y)\n",
    "    \n",
    "y, loss, cur_seq = test_prior_model_results()\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39c12e-7c42-447f-9035-d5d4c6285f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video_line(cur_seq.squeeze().numpy(), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81fa1b-2278-4cb5-a448-30f2c681aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video_line(y.squeeze().numpy(), 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d7e0b-73f3-4ccc-9c4f-4ea6af549769",
   "metadata": {},
   "source": [
    "## Sample AR Video Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f6db1-ab4f-49e6-9c0e-91f8ab2fefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SampleAutoRegressiveVideoCallback(pl.Callback):\n",
    "    def __init__(self, val_set, video_path=\"./val_videos/\"):\n",
    "        super().__init__()\n",
    "        self.val_set = val_set\n",
    "        self.val_count = 0\n",
    "        self.val_path = video_path\n",
    "        if not os.path.exists(self.val_path):\n",
    "            os.makedirs(self.val_path)\n",
    "\n",
    "    def generate_video(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        sample_idx = random.randint(0, len(self.val_set)-1)\n",
    "\n",
    "        x, y = self.val_set[sample_idx]\n",
    "        x = x.unsqueeze(0).to(pl_module.device)\n",
    "        y = y.unsqueeze(0).to(pl_module.device)\n",
    "        \n",
    "        _, cur_seq = pl_module.step(x, y)\n",
    "\n",
    "        # convert to numpy\n",
    "        x = x.squeeze(0).cpu().numpy()\n",
    "        y = y.squeeze(0).cpu().numpy()\n",
    "        y_hat = cur_seq.squeeze(0).cpu().numpy()\n",
    "\n",
    "        gif_path = os.path.join(self.val_path, f\"val_ar_video_{self.val_count}.gif\")\n",
    "\n",
    "        show_gif(x, y, y_hat, out_path=gif_path)\n",
    "        self.val_count += 1\n",
    "\n",
    "        return gif_path\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_rank == 0:\n",
    "            gif_path = self.generate_video(pl_module)\n",
    "            trainer.logger.experiment.log({\n",
    "                \"val_video\": wandb.Video(gif_path, fps=4, format=\"gif\")\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fec28-c98e-4a13-a941-66ad5964c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test video callback\n",
    "sample_video_cb = SampleAutoRegressiveVideoCallback(pl_module.val_set)\n",
    "gif_path = sample_video_cb.generate_video(pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9aaf04-2f9b-4925-8de8-dedff0cfe7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
