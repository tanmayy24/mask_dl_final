{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Trainer for MaskSimVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "from maskpredformer.mask_simvp import MaskSimVP, DEFAULT_MODEL_CONFIG\n",
    "from maskpredformer.vis_utils import show_gif\n",
    "from maskpredformer.simvp_dataset import DLDataset, DEFAULT_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MaskSimVPModule(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 in_shape, hid_S, hid_T, N_S, N_T, model_type,\n",
    "                 batch_size, lr, weight_decay, max_epochs,\n",
    "                 data_root, pre_seq_len=11, aft_seq_len=11,\n",
    "                 drop_path=0.0, unlabeled=False, downsample=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MaskSimVP(\n",
    "            in_shape, hid_S, hid_T, N_S, N_T, model_type, downsample=downsample, drop_path=drop_path,\n",
    "            pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len\n",
    "        )\n",
    "        self.train_set = DLDataset(data_root, \"train\", unlabeled=unlabeled, pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len)\n",
    "        self.val_set = DLDataset(data_root, \"val\", pre_seq_len=pre_seq_len, aft_seq_len=aft_seq_len)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, \n",
    "            num_workers=8, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def step(self, x, y):\n",
    "        y_hat_logits = self.model(x)\n",
    "        return y_hat_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat_logits = self.step(x, y)\n",
    "        \n",
    "        # Flatten batch and time dimensions\n",
    "        b, t, *_ = y_hat_logits.shape\n",
    "        y_hat_logits = y_hat_logits.view(b*t, *y_hat_logits.shape[2:])\n",
    "        y = y.view(b*t, *y.shape[2:])\n",
    "\n",
    "        loss = self.criterion(y_hat_logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat_logits = self.step(x, y)\n",
    "\n",
    "        # Flatten batch and time dimensions\n",
    "        b, t, *_ = y_hat_logits.shape\n",
    "        y_hat_logits = y_hat_logits.view(b*t, *y_hat_logits.shape[2:])\n",
    "        y = y.view(b*t, *y.shape[2:])\n",
    "       \n",
    "        loss = self.criterion(y_hat_logits, y)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.lr, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, max_lr=self.hparams.lr,\n",
    "            total_steps=self.hparams.max_epochs*len(self.train_dataloader()),\n",
    "            final_div_factor=1e4\n",
    "        )\n",
    "        opt_dict = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\":{\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            } \n",
    "        }\n",
    "\n",
    "        return opt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test out the MaskSimVP Lightning Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_module = MaskSimVPModule(\n",
    "    **DEFAULT_MODEL_CONFIG, \n",
    "    batch_size=1, lr=1e-3, weight_decay=0.0, max_epochs=10,\n",
    "    downsample=True, pre_seq_len=11, aft_seq_len=1,\n",
    "    data_root=\"../data/DL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pl_module(pl_module):\n",
    "    x, y = pl_module.val_set[0]\n",
    "    x = x.unsqueeze(0).to(pl_module.device)\n",
    "    y = y.unsqueeze(0).to(pl_module.device)\n",
    "    loss = pl_module.training_step((x,y), 0)\n",
    "    print(loss)\n",
    "test_pl_module(pl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Video Callback\n",
    "\n",
    "> sample video callback to generate video samples during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SampleVideoCallback(pl.Callback):\n",
    "    def __init__(self, val_set, video_path=\"./val_videos/\"):\n",
    "        super().__init__()\n",
    "        self.val_set = val_set\n",
    "        self.val_count = 0\n",
    "        self.val_path = video_path\n",
    "        if not os.path.exists(self.val_path):\n",
    "            os.makedirs(self.val_path)\n",
    "\n",
    "    def generate_video(self, pl_module):\n",
    "        pl_module.eval()\n",
    "        sample_idx = random.randint(0, len(self.val_set)-1)\n",
    "        \n",
    "        x, y = self.val_set[sample_idx]\n",
    "        x = x.unsqueeze(0).to(pl_module.device)\n",
    "        y = y.unsqueeze(0).to(pl_module.device)\n",
    "\n",
    "        y_hat_logits = pl_module.step(x,y).squeeze(0) # (T, 49, H, W)\n",
    "        y_hat = torch.argmax(y_hat_logits, dim=1) # (T, H, W)\n",
    "\n",
    "        # convert to numpy\n",
    "        x = x.squeeze(0).cpu().numpy()\n",
    "        y = y.squeeze(0).cpu().numpy()\n",
    "        y_hat = y_hat.cpu().numpy()\n",
    "\n",
    "        gif_path = os.path.join(self.val_path, f\"val_video_{self.val_count}.gif\")\n",
    "\n",
    "        show_gif(x, y, y_hat, out_path=gif_path)\n",
    "        self.val_count += 1\n",
    "\n",
    "        return gif_path\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_rank == 0:\n",
    "            gif_path = self.generate_video(pl_module)\n",
    "            trainer.logger.experiment.log({\n",
    "                \"val_video\": wandb.Video(gif_path, fps=4, format=\"gif\")\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test video callback\n",
    "sample_video_cb = SampleVideoCallback(pl_module.val_set)\n",
    "gif_path = sample_video_cb.generate_video(pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
